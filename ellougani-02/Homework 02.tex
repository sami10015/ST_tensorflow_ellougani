\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Homework 02}
\author{Sami Ellougani}
\date{October 22nd, 2017}

\begin{document}
\maketitle

\section{Results}

\begin{table}[h!]
\centering
\begin{tabular}{||c c c c c c c||} 
 \hline
 Neurons/Learning Rate & 0.00001 & 0.0001 & 0.001 & 0.01 & 0.1 & 1.0 \\ [0.5ex] 
 \hline\hline
 16 & 0.1131 & 0.3321 & 0.8093 & 0.9123 & 0.8768 & 0.1121 \\ 
 32 & 0.1076 & 0.6238 & 0.8979 & 0.9476 & 0.8421 & 0.1001\\
 64 & 0.2396 & 0.7856 & 0.9021 & 0.9531 & 0.8690 & 0.1304\\
 128 & 0.4301 & 0.8467 & 0.9137 & 0.9689 & 0.3412 & 0.1209\\
 256 & 0.5176 & 0.9010 & 0.9430 & 0.9690 & 0.5433 & 0.1101\\ 
 512 & 0.6198 & 0.9435 & 0.9476 & 0.4398 & 0.1239 & 0.1003\\
 1024 & 0.7893 & 0.9477 & 0.9501 & 0.3209 & 0.1001 & 0.1109\\
 2048 & 0.7932 & 0.9503 & 0.9494 & 0.2198 & 0.0879 & 0.1001\\
 4096 & 0.8436 & 0.9327 & 0.9386 & 0.1593 & 0.1200 & 0.0981\\ [1ex]
 \hline
\end{tabular}
\label{table:1}
\end{table}

\section{Discussion}
\subsection{Overview}
In this lab, I applied RNNs/LSTMs to the MNIST dataset to gain a better understanding of what metrics are best used for these types of neural networks. I varied vthe number of hidden units exponentially as follows, 16, 32, 64, 128, 256, 512, 1024, 2048, and 4096; and also varied the learning rate exponentially as follows: 0.00001, 0.0001, 0.001, 0.01, 0.1, and 1.0. 

\subsection{Results Analysis}
If you look at the results, you can clearly see that as the Learning Rate gets higher, the accuracy lowers tremendously. My prediction for this is because the neurons within the LSTM/RNN are not trained enough to gain a higher accuracy, as the jumps of training is much higher. Also, one thing I noticed, was that as you use more neurons, it takes much longer to train the LSTM/RNN. 

\end{document}